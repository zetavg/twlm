# sample.yaml
# This is sample configuration for demonstration and development purposes.
# It's not ment to be use for training actual models.

# To build the tokenizer with this config, run: `python build_tokenizer.py --cfg=sample`.

# Project and group name, used for logging and generating names for the outputs
# (when output names are not specified).
project_name: 'zh-tw-llm-dev'
group_name: 'sample'

# Base model and tokenizer name.
base_tokenizer_name: 'EleutherAI/pythia-70m'
base_model_name: 'EleutherAI/pythia-70m'

# Name of the builded tokenizer. If not specified, a name will be
# automatically generated based on the parameters used.
# This will also be use to load the tokenizer while preparing the
# dataset and training the model.
# tokenizer_name: ''

# Hugging Face user or organization name.
# This is used to save and load models and datasets from Hugging Face Hub.
hf_user_or_org_name: 'zh-tw-llm-dv-dv'

# Configuration for building the tokenizer.
tokenizer:
  # How to build the tokenizer. Supported values:
  #  - `word_frequency_list`: Insert new words from a word frequency list.
  build_with: word_frequency_list

  # How many tokens to add.
  tokens_to_add: 8000

  # Settings for `build_with: word_frequency_list`.
  word_frequency_list_settings:
    # The word frequency list dataset from Hugging Face Hub.
    word_frequency_list_name: 'zetavg/tw-sinica-corpus-word-frequency'

    # Additional words to include. Use this to add words that are not in the
    # word frequency list.
    include_words: [
      '。', '，', '、', '？', '！', '；', '：', '……', '～',
      '「', '」', '『', '』', '【', '】', '〖', '〗',
      '（', '）', '〔', '〕', '［', '］', '｛', '｝',
      '《', '》', '〈', '〉',
      '——', '──', '－', '−', '＿',
      '・', '．', '·',
      '／', '＼', '｜',
      '＜', '＞',
    ]

    # Rules for filtering out or replacing words.
    replace_rules:
      # Remove corrupted words.
      - { match: { regex: '�' }, replace: null }
      # 排除大多數專有名詞。
      - match:
          pos: ['Nb', 'FW', null]
        replace: null
        except: ['奧運', '中共', '國民黨', '民進黨', '新黨', '共產黨', '媽祖', '耶穌']
      # 排除數字所組成的詞，例如「一百八十」、「七零年代」。
      - match:
          regex:
            - '^[Ａ-Ｚａ-ｚ０-９﹒• ]+$'
            - '^[零一二兩三四五六七八九十廿卅百千萬億兆壹貳參肆伍陸柒捌玖拾佰仟０-９﹒•]{2,}$'
            - '^（[零一二兩三四五六七八九十廿卅百千萬億兆壹貳參肆伍陸柒捌玖拾佰仟０-９﹒•]+）$'
            - '^[第數][零一二兩三四五六七八九十百千萬億兆０-９﹒•]+$'
            - '^[零一二兩三四五六七八九十廿卅百千萬億兆０-９﹒•]+分之[零一二兩三四五六七八九十廿卅百千萬億兆０-９﹒•]+$'
            - '^[零一二兩三四五六七八九十廿卅百千萬億兆０-９﹒•]+[多餘來幾成次年月日天時分點世代歲起段樓％]$'
            - '^[零一二三四五六七八九十廿卅０-９]+(月份|年代?|世紀|學?年度|年級)$'
            - '^(星期|週|周)[一二三四五六日]$'
        replace: null
        except: ['十分', '一起', '一點', '一時', '千萬', '兩三', '百分之百']
      # 將例如「現代化」轉換成「現代」，如此一來 tokenizer 會將「現代化」分為「現代」+「化」，
      # 令模型得以學會「◯◯化」的用法，而非只是記下「現代化」這個詞彙。
      - match:
          pos: 'VHC'
          regex: '^(.{2,})化$'
        sub: '\1'
      # 將例如「羽球場」轉換成「羽球」，如此一來 tokenizer 會將「羽球場」分為「羽球」+「場」，
      # 令模型得以學會「◯◯場」的用法，而非只是記下「羽球場」這個詞彙。
      - match:
          regex: '^(.+球)場$'
        sub: '\1'
      # 同理，將例如「科學園區」轉換成「科學」。
      - match:
          pos: 'Nc'
          regex: '^(.{2,})園區$'
        sub: '\1'
      # 將例如「高雄市」轉換成「高雄」，如此一來 tokenizer 會將「高雄市」分為「高雄」+「市」，
      # 使模型不用分別學會「高雄」以及「高雄市」、有可能可以學會「◯◯市」的用法，
      # 也節省了 token 的數量。
      - match:
          pos: 'Nc'
          regex: '^(.{2,})[鄉鎮縣市區]$'
        sub: '\1'
      # 同理，但排除例如「市議會」等，「市議」一詞通常不會單獨出現的狀況。
      - match:
          pos: 'Nc'
          regex: '^(.{2,})[界院部會署局館系所]$'
        sub: '\1'
        except: ['委員會', '研究所', '中研院', '國科會', '資策會', '經建會', '工研院', '電信總局', '鎮公所', '事務所', '交易所', '農委會', '鄉公所', '地檢署', '警分局', '派出所', '托兒所', '消基會', '文建會', '兩廳院', '陸委會', '市議會']
      # "外國人", "美國人", "領導人"
      - match:
          pos: 'Na'
          regex: '^(.{2,})人$'
        sub: '\1'
        except: ['年輕人', '負責人', '投資人', '候選人', '一家人', '當地人', '製作人']
      # "藝術家", "科學家", "考古學家"
      - match:
          pos: 'Na'
          regex: '^(.{2,3})學?家$'
        sub: '\1'
        except: ['女人家', '婦人家', '新儒家', '窮人家', '縱橫家', '老人家', '老東家', '闊人家', '大戶人家', '婦道人家', '小戶人家', '水上人家', '諸子百家']
      # "建築師", "設計師", "工程師", "總工程師"
      - match:
          pos: 'Na'
          regex: '^副?總?([^副總]{2,})師$'
        sub: '\1'
        except: ['中醫師', '囝仔師', '正機師', '準教師', '獸醫師', '班導師', '練馬師', '總舖師', '老像師', '新三十師', '至聖先師', '音樂大師']
      # "行政院長", "教育部長"
      - match:
          pos: 'Na'
          regex: '^[原前]?(?:代|代理)?副?總?([^前代副總議警里首院部署局廳司處科組課股]{2,})[院部署局廳司處科組課股]?次?長$'
        sub: '\1'
        except: ['董事長', '理事長', '秘書長', '執行長', '分局長', '縣市長', '一技之長', '省市長', '負成長', '高成長', '大家長', '小組長', '區組長', '低成長', '偵一組長', '停管隊長', '考選部長', '年增長', '正成長', '支店長', '公賣局長', '中宣部長', '小市長']
      # "研究員", "技術人員"
      - match:
          pos: 'Na'
          regex: '^副?總?正?([^副總正議委人隊]{2,})[委人隊]?員$'
        sub: '\1'
        except: ['主跑員', '乘務員', '佐理員', '共黨員', '外務員', '從業員', '特派員', '義服員', '銜道員', '啦啦隊員', '指服團員']
      # "副總經理", "副主席"
      - match:
          pos: 'Na'
          regex: '^副(.{2,})$'
        sub: '\1'
        except: ['副作用']
      # 其他轉換規則，避免加入太多像是「大規模」之類的詞，因為模型可以自行學會「大」+「規模」。
      - { match: '大規模', replace: '規模' }
      - { match: '廢棄物', replace: '廢棄' }
      - { match: '共產主義', replace: '共產' }
      - { match: '資本主義', replace: '資本' }
      # 改變一些詞彙的 merge rule。例如在限制 12,000 個 token 時，詞彙表中會有
      # 「舊金山」但不會有「舊金」也不會有「金山」，為了加入「舊金山」這個詞，預設的做法是
      # 取前面先加入「舊金」一詞，再加入「舊金山」，但這樣就失去了同時加入「金山」(地名) 一詞
      # 的機會，所以這邊以特殊規則讓「舊金山」一詞分為「金山」及「舊金山」。
      - { match: '進一步', replace: ['一步', '進一步'] }
      - { match: '差不多', replace: ['不多', '差不多'] }
      - { match: '忍不住', replace: ['不住', '忍不住'] }
      - { match: '不見得', replace: ['見得', '不見得'] }
      - { match: '有助於', replace: ['助於', '有助於'] }
      - { match: '舊金山', replace: ['金山', '舊金山'] }
      - { match: '大躍進', replace: ['躍進', '大躍進'] }
      - { match: '半導體', replace: ['導體', '半導體'] }
      - { match: '總幹事', replace: ['幹事', '總幹事'] }
      - { match: '兩廳院', replace: ['廳院', '兩廳院'] }

# Training. Mutiple trains can be defined, with the names as the keys.
# Here we have two defined: 'only_embeddings' and 'with_peft'.
#
# To run the 'only_embeddings' training, first prepare the dataset with
# `python prepare_dataset.py --cfg=sample only_embeddings`, then start the training
# with `python train.py --cfg=sample only_embeddings`.
# Same applies to 'with_peft'.
training:
  # Define the training with the name called 'only_embeddings'.
  only_embeddings:
    # A suffix to append to the run name. This is useful for distinguishing
    # like different hardware setups or hyperparameters used.
    # Note that this will not be used if 'run_name' is explicitly specified.
    run_name_suffix: 'dev-05'

    # Name of the run. If not specified, one will be automatically
    # generated from the parameters used, and append with the 'run_name_suffix'.
    # run_name: ''

    # Name of the output model. If not specified, one will be automatically
    # generated from the parameters used.
    # model_name: ''

    # The maximum length of tokens for one training input. Training samples
    # exceeding this length will be truncated.
    # This should not exceed the maximum input length of the base model.
    max_tokens_length: 2048

    # Name of the intermediate dataset that will be generated and used for
    # training. If not specified, one will be automatically generated.
    # dataset_name: ''

    # Settings for the training dataset.
    dataset:
      # How the dataset should be built. Supported values:
      #  - `translations`: Translations between two languages.
      #  - `aplaca`: Alpaca-styled examples for instruction tuning.
      build_with:
        - translations
        - wikipedia
        - sharegpt
        - alpaca

      # A 'preview' column will be added to the dataset as non-encoded text
      # for the convenience to check what is contatined in a row of the dataset.
      # This controls the maximum length of the preview.
      preview_length: 256

      # Settings for `build_with` `translations`.
      translations_settings:
        source_dataset: 'zetavg/coct-en-zh-tw-translations-twp-300k'
        lang_1_key: 'en'
        lang_2_key: 'ch'
        templates:
          - "English: {lang_1}\nChinese: {lang_2}"
          - "Chinese: {lang_2}\nEnglish: {lang_1}"
        rows_limit: 100  # Limit the number of rows to use
        # Optional, for test split
        test_size: 0.1
        test_split_seed: 42
        test_rows_limit: 10  # Only works if test_size is set

      # Settings for `build_with` `wikipedia`.
      wikipedia_settings:
        # source_dataset: 'zetavg/zh-tw-wikipedia'
        # Use a dev dataset that contains only 1000 items
        source_dataset: 'zetavg/zh-tw-wikipedia-dev'
        exclude:
          - content_length_longer_than: 512
          - match: '小行星'
            in: 'markdown'
            in_range: [0, 40]
          - match: '，是中國'
            in: 'markdown'
            in_range: [0, 20]
          - match: '中華人民共和國'
            in: 'markdown'
            in_range: [0, 20]
          - match: '是中華人民共和國'
            in: 'markdown'
            in_range: [0, 40]
        rows_limit: 100  # Limit the number of rows to use
        # Optional, for test split
        test_size: 0.1
        test_split_seed: 42
        test_rows_limit: 10  # Only works if test_size is set

      # Settings for `build_with` `sharegpt`.
      sharegpt_settings:
        source_dataset: 'zetavg/ShareGPT-Processed'
        # The alpaca template to use. Supported values: `null` (default), `short`.
        train_on_inputs: false
        languages:
          - en: 0.4  # Limit the number of rows to use, int (> 1) means excect number while float (0 < n < 1) means percentage, float can only be used if rows_limit and test_rows_limit is used.
          - zh_Hant  # No limit
        rows_limit: 100  # Limit the total number of rows to use
        # Optional, for test split
        test_size: 0.1
        test_split_seed: 42
        test_rows_limit: 10  # Only works if test_size is set

      # Settings for `build_with` `alpaca`.
      alpaca_settings:
        source_dataset: 'zetavg/traditional-chinese-alpaca-en-align'
        # The alpaca template to use. Supported values: `null` (default), `short`.
        template: short
        train_on_inputs: false
        rows_limit: 100  # Limit the number of rows to use
        # Optional, for test split
        test_size: 0.1
        test_split_seed: 42
        test_rows_limit: 10  # Only works if test_size is set

    # Only train parameters which with name matches with one of the listed
    # regexp. If not specified, all parameters will be trained.
    only_train_parameters_matching:
      - 'embed'

    # Training arguments.
    # See: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
    training_arguments:
      # Train hyperparams
      num_train_epochs: 1
      per_device_train_batch_size: 2
      # auto_find_batch_size: true
      gradient_accumulation_steps: 1
      optim: adamw_torch
      learning_rate: 0.00005 # 5e-5
      lr_scheduler_type: constant
      warmup_steps: 100
      # Steps & save
      logging_steps: 10
      eval_steps: 50
      save_steps: 100
      save_total_limit: 1

    # Optional, controls the frequency of model output logging.
    # log_output_every_n_steps: 20

  # Define another training with the name called 'with_peft'.
  with_peft:
    # A suffix to append to the run name. This is useful for distinguishing
    # like different hardware setups or hyperparameters used.
    # Note that this will not be used if 'run_name' is explicitly specified.
    run_name_suffix: 'dev-052'

    # To train with PEFT (Parameter-Efficient Fine-Tuning), specify a method.
    use_peft: lora

    # Train on top of a model output from another training.
    base_on:
      # Start the training base on the output of the 'only_embeddings' training.
      output_of: only_embeddings

    dataset:
      # Use the same dataset as the 'only_embeddings' training.
      same_as: only_embeddings

    # LoRA config for `use_peft: lora`.
    # See: https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.LoraConfig
    lora_config:
      task_type: CAUSAL_LM
      r: 128
      lora_alpha: 64
      lora_dropout: 0.05
      target_modules:
        - 'embed'
        - 'query_key_value'
        - 'dense'

    # Training arguments.
    # See: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
    training_arguments:
      # Train hyperparams
      num_train_epochs: 1
      per_device_train_batch_size: 2
      # auto_find_batch_size: true
      gradient_accumulation_steps: 1
      optim: adamw_torch
      learning_rate: 0.00005 # 5e-5
      lr_scheduler_type: constant
      warmup_steps: 100
      # Steps & save
      logging_steps: 10
      eval_steps: 50
      save_steps: 100
      save_total_limit: 1


# Hugging Face options.
# This is necessary for training in the cloud since we rely on the
# Hugging Face Hub to store the intermediate tokenizer and dataset for servers
# in the cloud to download them from.
push_outputs_to_hf: false

# Weights & Biases settings.
report_to_wandb: true
wandb_project: zh-tw-llm-dev
wandb_group: sample
